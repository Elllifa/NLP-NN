{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 16Copy of task_unsolved_updated.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (Ubuntu Linux)",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "aS43kWID0rnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text classification with a 2-layer neural network"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "sA8S9ss80rnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Task description"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "Bf8ljM9m0rnR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text classification with neural networks\n",
        "In the following sections, you will implement a simple neural network for text classification and train it on the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/). \n",
        "The goal of text classification is, given an input text, to assign to it one of $K$ mutually exclusive labels. The most basic neural network consists of several feedforward fully-connected layers. Since text is a sequence of tokens (or characters), the most commonly used neural networks in NLP are [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network), which learn well from sequential data. However, regular feedforward neural networks are still widely used as baselines or parts of more complex models. In this assignment, you will implememnt a simple feed forward neural network and train it with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) on a subset of the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/).\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "5wy_L_QJ0rnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 20 Newsgroups dataset\n",
        "This dataset is a popular benchmark for classification models. It consists of $20000$ newsgroups documents, where each document has an associated label, selected out of $K=20$ classes (different newsgroups). Each class has roughly the same number of documents, making this dataset balanced. For for faster computation, here we will only use a small subset of classes, which consists of the following newsgroups: `comp.os.ms-windows.misc`, `rec.motorcycles`, `sci.space`, and `talk.politics.misc`. Each group contains roughly training $500$ samples. Thus, in total we will have a dataset with $4$ classes and slightly over $2000$ samples.\n",
        "\n",
        "[Scikit-learn library](https://scikit-learn.org) has a convenient way of loading the 20 Newsgroups dataset, so we will use it to fetch the dataset and look at the document representation."
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "id": "1jj4b_oe0rnT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Execute the cell below to make sure your environment is fully set up. You are free to import any additional [standard Python libraries](https://docs.python.org/3/library/), however, the notebook can be completed without doing so. External libraries other than the ones listed below are not permitted."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ieTfZ3s80rnU",
        "colab_type": "code",
        "outputId": "2062497e-0a1a-4a02-89f5-1196284e3f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q papermill\n",
        "import random\n",
        "import string\n",
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import papermill as pm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    21% |██████▉                         | 10kB 15.2MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 20kB 21.2MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▍           | 30kB 27.2MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████▏    | 40kB 3.8MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.46 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dUVeXZDi0rnW",
        "colab_type": "code",
        "outputId": "a430b09d-9d47-4dd4-bc05-827c161c4b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "categories = ['comp.os.ms-windows.misc', 'rec.motorcycles', 'sci.space', 'talk.politics.misc', ]\n",
        "dataset = fetch_20newsgroups(subset='train', categories=categories, data_home='.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "46RugwPH0rnX",
        "colab_type": "code",
        "outputId": "54ad7897-b284-4231-d6c2-6a64eb9c7449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('Samples:', len(dataset.data))\n",
        "print('Categories:', len(dataset.target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples: 2247\n",
            "Categories: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "t_8_UxWx0rnZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's print a random example to see what it looks like."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "MShzl53O0rnZ",
        "colab_type": "code",
        "outputId": "28345002-4f2a-4365-e59e-350fd386cd66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "cell_type": "code",
      "source": [
        "random_sample_idx = random.randrange(len(dataset.data))\n",
        "print('Label:', dataset.target_names[dataset.target[random_sample_idx]], '\\n')\n",
        "print(dataset.data[random_sample_idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: sci.space \n",
            "\n",
            "From: nsmca@aurora.alaska.edu\n",
            "Subject: Moon Colony Prize Race! $6 billion total?\n",
            "Lines: 26\n",
            "Nntp-Posting-Host: acad3.alaska.edu\n",
            "Organization: University of Alaska Fairbanks\n",
            "\n",
            "I think if there is to be a prize and such.. There should be \"classes\"\n",
            "such as the following:\n",
            "\n",
            "Large Corp.\n",
            "Small Corp/Company (based on reported earnings?)\n",
            "Large Government (GNP and such)\n",
            "Small Governemtn (or political clout or GNP?)\n",
            "Large Organization (Planetary Society? and such?)\n",
            "Small Organization (Alot of small orgs..)\n",
            "\n",
            "The organization things would probably have to be non-profit or liek ??\n",
            "\n",
            "Of course this means the prize might go up. Larger get more or ??\n",
            "Basically make the prize (total purse) $6 billion, divided amngst the class\n",
            "winners..\n",
            "More fair?\n",
            "\n",
            "There would have to be a seperate organization set up to monitor the events,\n",
            "umpire and such and watch for safety violations (or maybe not, if peopel want\n",
            "to risk thier own lives let them do it?).\n",
            "\n",
            "Any other ideas??\n",
            "==\n",
            "Michael Adams, nsmca@acad3.alaska.edu -- I'm not high, just jacked\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "EYcafHOJ0rnc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text representation for neural networks"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "mCT1vTWy0rnc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the documents are quite long and diverse. However, documents within each class contain some specific words that are less likely to be used in the documents from other classes. Our model will leverage this fact to perform text classification. In order to do that, we need to convert each document into numeric form. One of the simplest and most popular representations in NLP is the *multi-hot representation*. In this representation, each document $d$ is represented as a vector $\\mathbf{x}$ of length $|V|$, where $V$ is the vocabulary - that is, a set of all possible words $v$, used in the whole dataset. The vector $\\mathbf{x}$ contains $1$ on the $i$th position if the corresponding word $v_i$ was used in the document $d$, and $0$ otherwise.\n",
        "\n",
        "Finally, we would need to convert the labels (`dataset.target`) to a matrix of one-hot vectors to use it to train the network."
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "P1xxs3fh0rnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The cell below will be used for unit tests. Please do not modify it."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "WbP4fmvl0rnd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "docs_toy = [\n",
        "\"\"\"\n",
        "Hi!\n",
        "\n",
        "How are you?\n",
        "\n",
        "\"\"\",\n",
        "\"\"\"\n",
        "Do you have a dog?\n",
        "\"\"\"\n",
        "]\n",
        "docs_toy_labels = np.array([0, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "3ltVzE470rng",
        "colab_type": "code",
        "outputId": "df6a2eee-db46-4a87-dac9-8ef84d56ff73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "print('First document:', docs_toy[0])\n",
        "print('Second document:', docs_toy[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First document: \n",
            "Hi!\n",
            "\n",
            "How are you?\n",
            "\n",
            "\n",
            "Second document: \n",
            "Do you have a dog?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "Dvc2tFAc0rni",
        "colab_type": "code",
        "outputId": "0841810e-8c3e-44ae-a665-b1c3fbfa8699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('Labels:\\n', docs_toy_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels:\n",
            " [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "fUAHVcxr0rnk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 \n",
        "#### (2 points)\n",
        "\n",
        "Complete the code in the `tokenize_doc` function that returns a list of tokens for a given document.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- You can use the `string.punctuation` to get the punctuation characters\n",
        "- You can use the `string.whitespace`  to get all whitespace characters\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "H2gPe-Xj0rno",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_doc(doc):\n",
        "    \"\"\"\n",
        "        Convert the input document into a list of tokens, discarding all punctuation and lowercasing the tokens\n",
        "        doc: string\n",
        "\n",
        "        return list of strings\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # discard all punctuation\n",
        "    doc2 = doc.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # replace all whitespace characters with just space\n",
        "    # split doc into tokens by space\n",
        "    # discard empty tokens and lowercase\n",
        "    doc2 = doc2.lower()\n",
        "    tokens = doc2.split()\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "atUKQFvA0rnr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "docs_toy_tokenized = [tokenize_doc(d) for d in docs_toy]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "a08UWOfd0rns",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you implememnted this function correcrtly, the documents would look like this:\n",
        "```\n",
        "['hi', 'how', 'are', 'you']\n",
        "```\n",
        "```\n",
        "['do', 'you', 'have', 'a', 'dog']\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "zomcXBDI0rnt",
        "colab_type": "code",
        "outputId": "7c4a421e-cb71-48b7-a35e-b64791041ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "print('First document tokenized:\\n', docs_toy_tokenized[0])\n",
        "print('Second document tokenized:\\n', docs_toy_tokenized[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First document tokenized:\n",
            " ['hi', 'how', 'are', 'you']\n",
            "Second document tokenized:\n",
            " ['do', 'you', 'have', 'a', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "LVr62Wu00rnv",
        "colab_type": "code",
        "outputId": "aeb5a05f-f738-462e-c90d-f388f4a38810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('tokenization.test_docs.0', docs_toy_tokenized[0])\n",
        "pm.record('tokenization.test_docs.1', docs_toy_tokenized[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "tokenization.test_docs.0": [
                "hi",
                "how",
                "are",
                "you"
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "tokenization.test_docs.1": [
                "do",
                "you",
                "have",
                "a",
                "dog"
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "_szun6N00rnz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2\n",
        "#### (1 point)\n",
        "\n",
        "Complete the code in `build_vocab`. If the `min_count` argument is `None`, you should not discard any tokens.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- Hint: you might find the `Counter` class from the `collections` library and `from_iterable` method from `itertools.chain` useful here."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "FSrq-KDq0rn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_vocab(docs, min_count=None):\n",
        "    \"\"\"\n",
        "        Build the vocaublary mapping (that is, the correspondance between the token and its numeric id)\n",
        "        docs: a list of tokenized documents\n",
        "        min_count (optional): int, discard tokens that appeared less than min_count times\n",
        "\n",
        "        return dictionary str -> int\n",
        "    \"\"\"\n",
        "\n",
        "    # count all tokens in all documents and filter those that appear less than min_count  \n",
        "    word_counter = Counter(x for xs  in docs for x in set(xs))\n",
        "    dictionary = dict(word_counter)\n",
        "\n",
        "    if min_count is not None:\n",
        "        pass \n",
        "        uniqueWords = []\n",
        "        for k, v in dictionary.items():\n",
        "          if v >= min_count:\n",
        "            uniqueWords.append(k)\n",
        "        docs_sorted = sorted(uniqueWords)    \n",
        "        \n",
        "    else:\n",
        "        pass \n",
        "        flat_list = [item for sublist in docs for item in sublist]\n",
        "        uniqueWords = []\n",
        "        for i in flat_list:\n",
        "          if not i in uniqueWords:\n",
        "            uniqueWords.append(i)\n",
        "        docs_sorted = sorted(uniqueWords)   \n",
        "    #create the vocabulary mapping\n",
        "    l = [i for i in range(len(docs_sorted))]\n",
        "    vocab = dict(zip(docs_sorted, l))\n",
        "        \n",
        "    return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7T_X1ppwJ0bD",
        "colab_type": "code",
        "outputId": "4ea19037-025d-4fc9-e094-e1104a38bb7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "flat_list = [item for sublist in docs_toy_tokenized for item in sublist]\n",
        "print(flat_list)\n",
        "d = sorted(flat_list)\n",
        "print(len(d))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'how', 'are', 'you', 'do', 'you', 'have', 'a', 'dog']\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "A6YvNbNu0rn1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(docs_toy_tokenized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "TRAbxCQg0rn2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you implememnted this function correcrtly, the vocabulary would contain 8 tokens and similar to the following:\n",
        "```\n",
        "{'a': 0, 'are': 1, 'do': 2, 'dog': 3, 'have': 4, 'hi': 5, 'how': 6, 'you': 7}\n",
        "```"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "h0kmK3xZ0rn3",
        "colab_type": "code",
        "outputId": "a006ebe1-f364-48da-c5f7-0b2ba8ea315a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('Vocabulary:\\n', vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            " {'a': 0, 'are': 1, 'do': 2, 'dog': 3, 'have': 4, 'hi': 5, 'how': 6, 'you': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h8Mk8c-M0rn5",
        "colab_type": "code",
        "outputId": "14319a7b-b6d5-4552-992c-a8285ae2abcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('vocab.test_docs', vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "vocab.test_docs": {
                "a": 0,
                "do": 2,
                "dog": 3,
                "how": 6,
                "hi": 5,
                "are": 1,
                "have": 4,
                "you": 7
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "R7GYG6sY0rn6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 \n",
        "#### (1 point)\n",
        "\n",
        "Complete the code in the `doc_to_multihot` function that tranforms the input document to its' multi-hot representation. \n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- Recall that multi-hot representation of a given document should return a vector containing ones at the positions of the words present in the document and zeros at all the other positions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZFQyukLC7STY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "7SWTta5r0rn7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def doc_to_multihot(doc, vocab):\n",
        "    \"\"\"\n",
        "        Convert a document to a multihot representation\n",
        "        doc: str, a tokenized document\n",
        "        vocab: dict, vocabulary mapping\n",
        "\n",
        "        return np.array, shape=(|V|,)\n",
        "    \"\"\"\n",
        "\n",
        "    # create a vector of zeros of the shape (|V|, )\n",
        "    x = np.zeros(len(vocab))\n",
        "\n",
        "    # set the corresponding dimensions to 1\n",
        "    for k,v in vocab.items():\n",
        "      for word in doc:\n",
        "        if k == word:\n",
        "          x[vocab[k]] = 1\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "FhMYlfWG0rn9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "docs_toy_multi_hot = [doc_to_multihot(doc, vocab) for doc in docs_toy_tokenized]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "3jOAgTMi0rn-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you implemented this function correcrtly, the documents in multi-hot representation would look similar to the following\n",
        "```\n",
        "[0. 1. 0. 0. 0. 1. 1. 1.]\n",
        "```\n",
        "```\n",
        "[1. 0. 1. 1. 1. 0. 0. 1.]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "HScYLp3O0rn_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how these documents have $1$ in the dimensions matching to the tokens indices from the vocabulary. In particular, since both documents have the token `you`, they both have $1$ in the corresponding dimension."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "clmjoT-D0roA",
        "colab_type": "code",
        "outputId": "0beeb9b9-b101-4a46-fbe0-e700f1de62d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "print('First document multi-hot:\\n', docs_toy_multi_hot[0])\n",
        "print('Second document multi-hot:\\n', docs_toy_multi_hot[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First document multi-hot:\n",
            " [0. 1. 0. 0. 0. 1. 1. 1.]\n",
            "Second document multi-hot:\n",
            " [1. 0. 1. 1. 1. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "HprY1J3l0roB",
        "colab_type": "code",
        "outputId": "89a1cad9-7f5e-4ec9-c984-798eab6d1121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('multihot.test_docs.0', docs_toy_multi_hot[0].tolist())\n",
        "pm.record('multihot.test_docs.1', docs_toy_multi_hot[1].tolist())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "multihot.test_docs.0": [
                0,
                1,
                0,
                0,
                0,
                1,
                1,
                1
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "multihot.test_docs.1": [
                1,
                0,
                1,
                1,
                1,
                0,
                0,
                1
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "Xtfqh-9Q0roC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 \n",
        "#### (2 points)\n",
        "\n",
        "Because later on it will be easier to work with true labels as with one-hot encoded vectors rather than with single integers, you will now need to complete the code in `labels_to_onehot` that converts a vector of class labels to a one-hot representation.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- For example, if there were $k=3$ classes in your dataset and the true labels vector looked like $\\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$ (assuming there were $N=4$ examples), then the ultimate one-hot representation of the labels would be the following: $\\mathbf{y} = \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0\\end{bmatrix}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "Uz0jxBRh0roD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def labels_to_onehot(labels):\n",
        "    \"\"\"\n",
        "        Convert the indices to one-hot representation\n",
        "        labels: np.array of labels, shape=(N,)\n",
        "\n",
        "        return np.array, shape=(N, k)\n",
        "       \"\"\"\n",
        "    #labels defines the columns of nonzero elements in the output array\n",
        "     l = [i for i in range(len(labels))]\n",
        "    unique= np.unique(labels)\n",
        "    vocab = dict(zip(unique, l))\n",
        "    \n",
        "    n_classes = len(set(labels))\n",
        "    n_samples = len(labels)\n",
        "    # create a matrix of zeros of shape (n_samples, n_classes)\n",
        "    one_hot = np.zeros((n_samples, n_classes))\n",
        "    # fill one-hot values\n",
        "    a = []\n",
        "    for label in labels:\n",
        "        for k,v in vocab.items():\n",
        "            if k == label:\n",
        "                a.append(vocab[k])\n",
        "    one_hot[np.arange(n_samples), a] = 1\n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "pmQeUuFt0roE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "docs_toy_labels_onehot = labels_to_onehot(docs_toy_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "OAMQsB5G0roH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you implemented this function correcrtly, the one-hot labels would look like this:\n",
        "```\n",
        "[[1. 0.]\n",
        " [0. 1.]]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "VyuFxEQ50roI",
        "colab_type": "code",
        "outputId": "1c49eb31-7488-45d7-f9c6-b2884d3be749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "print('Test labels one-hot:\\n', docs_toy_labels_onehot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test labels one-hot:\n",
            " [[1. 0.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "_2b1DyTD0roJ",
        "colab_type": "code",
        "outputId": "7fa841b7-a813-4141-b9b4-e4aba945a268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('onehot.test_labels', docs_toy_labels_onehot.tolist())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "onehot.test_labels": [
                [
                  1,
                  0
                ],
                [
                  0,
                  1
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "-Kxu85dO0roK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Neural networks overview"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "9AbPWDB10roL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "Neural networks (NNs) are machine learning alrogithms that are designed to capture patterns in a way similar to what human brain does. NNs are very efficient at discovering latent structures in unlabeled data and automatically extracting (or learning) relevant features.\n",
        "\n",
        "In principle, there exist various NN architectures that are commonly applied to different problems. Essentially, every neural network consists of a set of computational units called neurons. Every neuron processes the input signal, computes the output, and passes it further to the next neuron. Altogether neurons can recognize complex patterns. Our current task will be focused on the simplest type of a neural network - the so-called fully-connected neural network.\n",
        "\n",
        "### Fully-connected neural network\n",
        "Fully-connected neural networks can be viewed as a stack of layers, where every layer consists of an arbitrary number of neurons. Conventionally, the first layer is called an input layer, all the middle layers are referred to as hidden layers, and the last layer is called the output layer. The output layer is responsible for making ultimate high-level decisions and generating the predictions. The diagram of the feed-forward neural network with one layer (it is common to only count the hidden layers):"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "N0LG_H0M0roL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"data:image/png;base64,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\"></center>"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "lzOhYXVh0roM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The depth of a neural network is one of its key features. Each node of a single layer combines the outputs of the nodes in the previous layer and adds them up in different proportions, i.e. weights, and applies a non-linear function to the obtained value. Non-linearity is important, since a stack of linear layers without non-linear activations would be equivalent to just one single linear layer, which reduces the capacity of a neural network to recover the elaborate structures in the input data. Formally, if we denote the weights and the biases of the $i$-th layer as matrices $W^i$ and $\\mathbf{b}$ respectively, and the input example feature vector as $\\mathbf{x}$, then the $i$-th linear layer's transformation is as follows:\n",
        "\n",
        "\n",
        "$$ \\mathbf{z}^{(i)} = W^{(i)}\\mathbf{x} + \\mathbf{b} $$\n",
        "$$ \\mathbf{a}^{(i)} = f\\left(\\mathbf{z}^{(i)}\\right)$$\n",
        "\n",
        "where $f$ is the mentioned non-linear function called the activation function. There is a number of activation functions that are widely used in Deep Learning. In classification tasks, the dimension of the last layer equals the number of classes $k$ and the activation function of the last layer is, typically, either the sigmoid function (for binary classification problmes) or the softmax function (which is a generalization of the sigmoid function over multiclass classification problems).\n",
        "\n",
        "### Softmax function and predictions\n",
        "Given a vector $\\mathbf{z}$ of the dimension $k$, the softmax function computes the vector with the following components:\n",
        "\n",
        "$$ \\text{softmax}\\left(\\mathbf{z}\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}} $$\n",
        "\n",
        "In other words, the softmax function first exponentiates the vector (elementwise) and then normalizes it such that all the components would add up to 1. In the output layer the resulting vector can be interpreted as a probability distribution over the number of classes. The network then makes its prediction through selecting a class with the maximum associated probability:\n",
        "\n",
        "$$ \\hat{y}_{pred} = \\underset{j \\in 1..k}{\\text{argmax}}\\left[\\text{softmax}\\left(\\mathbf{z}\\right)\\right] $$\n",
        "\n",
        "One interesting property of the softmax function is that it is invariant to constant offsets, i.e. $\\text{softmax}\\left(\\mathbf{z}\\right) = \\text{softmax}\\left(\\mathbf{z} + \\mathbf{c}\\right)$, where $\\mathbf{c}$ is a broadcasted vector of equal constant values. \n",
        "\n",
        "To sum up, for a single-layer neural network the predicted probability distributions are computed as:\n",
        "\n",
        "$$ \\mathbf{\\hat{y}} = \\text{softmax}\\left(W\\mathbf{x} + \\mathbf{b}\\right) $$\n",
        "\n",
        "### Training\n",
        "Simply put, training of a neural network reduces to learning its preliminarily initialized weights such that the learned weights would optimize a given objective. The most popular optimization algorithm used in Deep Learning is gradient descent. The training objective is usually formulated as maximization of the likelihood of the training data, which for classification tasks is equivalent to minimizing the cross-entropy function:\n",
        "\n",
        "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n \\rightarrow \\min_{W, b}$$\n",
        "\n",
        "where $\\hat{y}_n$ is the predicted probability of the correct class $y_n$ for the $n$-th training example and $N$ is the total number of training examples. In order to reduce overfitting, an additional term penalizing large weights is added to the loss function as:\n",
        "\n",
        "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n + \\lambda \\Vert W\\Vert ^2 \\rightarrow \\min_{W, b}$$\n",
        "\n",
        "\n",
        "The actual training is done in iterations such that after every iteration the network produces the predictions, computes the value of the loss function based on the predictions and the ground-truth labels, calculates the gradients through a technique called backpropagation, and then uses the derived gradients to update the weights (a step of the gradient descent algorithm). Backpropagation is the heart of neural networks algorithm; it calculates the gradients of the loss function with respect to every trained weight through application of a series of chain rules. According to the gradient descent algorithm, the weights of the network are updated as:\n",
        "\n",
        "$$ W_{ij} \\leftarrow W_{ij} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} $$\n",
        "\n",
        "$$ b_{i} \\leftarrow b_{i} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial b_{i}} $$\n",
        "\n",
        "where $\\alpha$ is a hyperparameter called learning rate that adjusts the magnitude of the weight updates.\n",
        "\n",
        "In practice, training is done in batches: instead of processing one training instance at a time, a network receives multiple examples which are processed in parallel. Besides the computation efficiency, this approach also produces a better estimate of the gradients.\n",
        "\n",
        "---\n",
        "## Now let's code it up!\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "2cFMw9I00roM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Step-by-step single-layer network implementation"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "J2Yt-a_u0roO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The cell below will be used for unit tests. Please do not modify it."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "HHk2Bxd_0roP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# 5 features x 3 classes\n",
        "W_toy = np.array([[0.2, -0.3, 0.5],\n",
        "                 [-0.9, 0.6, 0.4],\n",
        "                 [0.1, 0.7, -0.2],\n",
        "                 [0.6, 0.1, -0.3],\n",
        "                 [0.7, -0.1, 0.6]])\n",
        "\n",
        "# 3 classes\n",
        "b_toy = np.array([[0.05, 0.1, -0.2]])\n",
        "\n",
        "# 4 examples x 5 features\n",
        "x_toy = np.array([[-1.0, -0.7, 0.3, 0.8, 0],\n",
        "                 [0.5, -0.2, 0.6, 0, -0.4],\n",
        "                 [0.1, 0, -0.4, -0.1, -0.2],\n",
        "                 [0.6, 0.7, 0.2, 0.1, 0.4]])\n",
        "\n",
        "# 3 classes one-hot\n",
        "y_true_toy = np.array([[0, 1, 0],\n",
        "                      [1, 0, 0],\n",
        "                      [0, 1, 0],\n",
        "                      [0, 0, 1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "sJ29r-8_0roR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1\n",
        "#### (1 point)\n",
        "\n",
        "Using the definition of a neural network linear layer, complete the code in `linear` function. Recall that\n",
        "\n",
        "$$ \\mathbf{z}^{(i)} = W^{(i)}\\mathbf{x} + \\mathbf{b} $$\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "\n",
        "- __From now on, all the computations should be done in batches__, i.e. instead of a single example vector with the shape $[1\\times d]$ the network will take batches with the shape $[m\\times d]$.\n",
        "- Note that numpy arrays support [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html): during arithmetic operations, smaller arrays get automatically reshaped to make the shapes of operands compatible. You can take advantage of broadcasting starting from the cell below and in the subsequent cells.\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "BsunsaGb0roR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear(x, W, b):\n",
        "    \"\"\"\n",
        "        Compute output of linear transformation given x, W, b\n",
        "        x: np.array, shape=[m, d] input\n",
        "        W: np.array, shape=[d, k] weights\n",
        "        b: np.array, shape=[1, k] biases\n",
        "\n",
        "        return np.array, shape=[m, k]\n",
        "    \"\"\"\n",
        "    # place your code here\n",
        "    #if we want to multiply 2 matrices then the number of columns \n",
        "    #of the first must be equal to the number of rows of the second\n",
        "    x_trans = x.transpose() #shape = d,m\n",
        "    \n",
        "    W_trans = W.transpose() #shape = k,d\n",
        "   \n",
        "    Wx = np.dot(W_trans, x_trans) #k,m\n",
        "    Wx = Wx.transpose()\n",
        "    #bb = b.transpose() #k,1\n",
        "\n",
        "    result = (Wx+b) #m,k  \n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dcm4ztyBOyUD",
        "colab_type": "code",
        "outputId": "9e367b36-0bb2-43f2-8c6f-8b2ac9aa12ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "x = np.array([1, 2])\n",
        "W1 = np.array([[1,1], \n",
        "               [2,3]])\n",
        "\n",
        "b = np.array([1,2])\n",
        "R = linear(x, W1, b)\n",
        "print(R)\n",
        "print(R.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6 9]\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "id": "Ec_mdrzC0roT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you implemented this function correctly, calling it on the following input\n",
        "\n",
        "$$\n",
        "W = \\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "2 & 3\n",
        "\\end{bmatrix},\n",
        "x = \\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix},\n",
        "b = \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "would produce the result below:\n",
        "\n",
        "$$\n",
        "z = W \\times x^\\intercal + b^\\intercal\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "6 & 9 \\\\\n",
        "\\end{bmatrix}^\\intercal\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "sOv74Dsc0roT",
        "colab_type": "code",
        "outputId": "787f947e-06f2-4870-cac8-9b6dc104760f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "z_toy = linear(x_toy, W_toy, b_toy)\n",
        "print('Z:\\n', z_toy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Z:\n",
            " [[ 0.99  0.27 -1.28]\n",
            " [ 0.11  0.29 -0.39]\n",
            " [-0.17 -0.2  -0.16]\n",
            " [-0.1   0.45  0.55]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "TSbyioij0roV",
        "colab_type": "code",
        "outputId": "af436628-912f-45f5-bd39-e3ae5067d0c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# submit\n",
        "pm.record('linear.z.toy', z_toy.tolist())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "linear.z.toy": [
                [
                  0.99,
                  0.27,
                  -1.28
                ],
                [
                  0.11000000000000003,
                  0.2899999999999999,
                  -0.39
                ],
                [
                  -0.16999999999999998,
                  -0.19999999999999998,
                  -0.16
                ],
                [
                  -0.09999999999999999,
                  0.44999999999999996,
                  0.5499999999999998
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "wjGBKKF10roX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 \n",
        "#### (1 point)\n",
        "Using the definition of the softmax function, complete the code in the `softmax` function. Recall that\n",
        "\n",
        "$$ \\text{softmax}\\left(\\mathbf{z}\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}} $$\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "\n",
        "- It is recommended to subtract the maximum from the input vector for each sample before the actual softmax implementation. This is done for computational stability and doesn't change the output of the softmax function due to its invariance to offsets property\n",
        "\n",
        "- You can verify your implementation by checking if the resulting probabilities add up to 1 for each example."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "1-6zZmoy0roY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "        Compute softmax values for each element (or score) in z\n",
        "        z: np.array, shape=[m, k]\n",
        "\n",
        "        return np.array, shape=[m, k]\n",
        "    \"\"\"\n",
        "    # place your code here\n",
        "    z1 = z - np.max(z)\n",
        "    z1 = np.exp(z1)\n",
        "    summa = np.sum(z1)\n",
        "    z1 = z1/summa\n",
        " \n",
        "    return z1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjUg9Vnq8LnD",
        "colab_type": "code",
        "outputId": "7484f527-866d-4984-96da-4cf88d286061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "z = np.array([6,9])\n",
        "print(softmax(z))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.04742587 0.95257413]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "id": "Q3rXUrEF0roZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you implemented this function correctly, calling it on the following input\n",
        "\n",
        "$$\n",
        "z = \\begin{bmatrix}\n",
        "6 & 9 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "would produce the result similar to the one below:\n",
        "\n",
        "$$\n",
        "y = \\text{softmax}(z) = \\begin{bmatrix}\n",
        "0.05 & 0.95 \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "CAyUti8w0rob",
        "colab_type": "code",
        "outputId": "c2dc5a56-3756-4899-9828-f21b690440e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "y_toy = softmax(z_toy)\n",
        "print('y:\\n', y_toy)\n",
        "print('y sum by columns:\\n', np.sum(y_toy, axis=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            " [[0.19046325 0.09270842 0.01967717]\n",
            " [0.0790009  0.09458125 0.04791647]\n",
            " [0.0597076  0.05794297 0.06030767]\n",
            " [0.06403689 0.11099213 0.12266527]]\n",
            "y sum by columns:\n",
            " [0.30284885 0.22149863 0.17795824 0.29769429]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "fF6dRsWW0rod",
        "colab_type": "code",
        "outputId": "c110159e-ea16-4d7c-da63-1e7ff50d47fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# submit\n",
        "pm.record('softmax.y.toy', y_toy.tolist())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "softmax.y.toy": [
                [
                  0.19046325399183844,
                  0.09270841855800445,
                  0.019677173995618066
                ],
                [
                  0.07900090305908333,
                  0.09458125284463748,
                  0.04791646985031961
                ],
                [
                  0.05970759809237507,
                  0.057942971887654814,
                  0.060307669429397796
                ],
                [
                  0.0640368874371109,
                  0.11099212840520717,
                  0.12266527244875296
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "K0juG0yl0roe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3\n",
        "#### (1 point)\n",
        "\n",
        "Compute the loss given the predicted probabilities and the target labels. Use the cross-entropy loss for calculation:\n",
        "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n$$\n",
        "where $\\hat{y}_n$ is the predicted probability of the correct class $y_n$ for the $n$-th training example, and $N$ is the total number of training examples. For one-hot-encoded labels representation, the equation becomes\n",
        "$$ \\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N \\sum_{j=1}^k y_n^j\\log \\hat{y}_n^j $$ where $y_n^j$ is one-hot encoded (1 at the position of the correct class and 0 elsewhere) and $\\hat{y}_n^k$ is a probability distribution over $k$ classes for the $n$-th training example.\n",
        "\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- To compute the probabilities, use the `softmax` function implemented above.\n",
        "- Use $e$ as a base for the logarithm.\n",
        "- It is also recommended to add a tiny `epsilon` to the predicted probabilites (inside the log of cross entropy). This helps to avoid taking logarithms of zero probabilities."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "pbI3zgDg0roe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_entropy(y, y_true, epsilon=1e-9):\n",
        "    \"\"\"\n",
        "        Compute the cross entropy loss function for model softmax output with respect to labels.\n",
        "\n",
        "        y: np.array, shape=[m, k] output predictions of softmax\n",
        "        y_true: np.array, shape=[m, k] one-hot encoded true labels\n",
        "        epsilon: float, small constant to add to input of log for computational stability\n",
        "\n",
        "        return: np.float, the value of the loss function\n",
        "    \"\"\"\n",
        "    # place your code here\n",
        "    in_sum = np.sum(y_true*np.log(y + epsilon), axis = 1)\n",
        "    ext_sum = np.sum(in_sum, axis = 0)\n",
        "    L = -1*ext_sum/np.size(y, 0)\n",
        "    return L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "r1G0_znv0roi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.4\n",
        "#### (1 point)\n",
        "\n",
        "Complete the code in `regularization` that adds a regularization term to the loss function. Your code also should compute the gradient of the loss with respect to the weights (see the formula to derive the mathematical expression for the derivative).\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "\n",
        "- Recall that regularization is computed as $\\lambda \\Vert W\\Vert ^2$."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ik9JTm-w0roi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def regularization(params, lam=0.05):\n",
        "    \"\"\"\n",
        "        Compute gradients for squared weight regularization. A list of parameters\n",
        "        should be provided to this function (for example, [weight, bias] for a linear network). This regularization\n",
        "        should force parameters values to be small, keeping the overall model more simple. Here, a parameter is a grouping\n",
        "        of weights or biases in a single numpy array. Loss value and gradients are calculated together for simplicity.\n",
        "\n",
        "        params: list of np.arrays, each a parameter (matrix or vector) of the model to be regularized\n",
        "        lam: float, lambda coefficient multiplied by regularization term\n",
        "\n",
        "        Return: float value of regularization loss, list of np.array gradients, one per each param\n",
        "    \"\"\"\n",
        "    gradients = []\n",
        "    value = 0\n",
        "    \n",
        "    # place your code here to update `value` and `gradients`\n",
        "    for a in params:\n",
        "      value += np.sum(np.square(a))*lam\n",
        "      grad = 2*lam*a\n",
        "      gradients.append(grad)\n",
        "    return value, gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "44i3A6pXTheE",
        "colab_type": "code",
        "outputId": "8049f09f-e9d7-4ed1-846b-b778662ab644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "xent_toy = cross_entropy(y_toy, y_true_toy)\n",
        "print('Cross entropy:', xent_toy)\n",
        "value, gradients = regularization([W_toy,b_toy], lam=0.05)\n",
        "print('L2 regularization: %s' % value)\n",
        "print('L2 regularization: %s' % gradients)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross entropy: 2.465795983239012\n",
            "L2 regularization: 0.18112499999999995\n",
            "L2 regularization: [array([[ 0.02, -0.03,  0.05],\n",
            "       [-0.09,  0.06,  0.04],\n",
            "       [ 0.01,  0.07, -0.02],\n",
            "       [ 0.06,  0.01, -0.03],\n",
            "       [ 0.07, -0.01,  0.06]]), array([[ 0.005,  0.01 , -0.02 ]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "OIbVAhkg0roj",
        "colab_type": "code",
        "outputId": "56bd1026-1241-45c8-bea8-edb5018c6e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "xent_toy = cross_entropy(y_toy, y_true_toy)\n",
        "print('Cross entropy:', xent_toy)\n",
        "value, _ = regularization([W_toy, b_toy], lam=0.05)\n",
        "print('L2 regularization: %s' % value)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross entropy: 2.465795983239012\n",
            "L2 regularization: 0.18112499999999995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "FH_UTLY20rol",
        "colab_type": "code",
        "outputId": "dbfd1dff-42ec-47bb-f270-79424e73db9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# submit\n",
        "pm.record('cross_entropy.xent.toy', xent_toy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "cross_entropy.xent.toy": 2.465795983239012
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "K9NsRwN20roo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that the forward pass is complete, we can start backpropagating the error. Recall that our goal is to find $\\frac{\\partial \\mathcal{L}}{\\partial W}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}$ to perform the weight updates. We can expand these expressions using the chain rule as:\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial W}$$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}}$$\n",
        "where $\\mathbf{z}$ is the linear layer output and $\\mathcal{L}$ is the loss function. "
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "ZD9-r_EL0roo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5\n",
        "#### (3 points)\n",
        "Knowing that $\\text{softmax}\\left(\\mathbf{z}\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}}$, $\\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{z})$ and that $\\mathcal{L} = - \\frac{1}{N}\\sum_{n=1}^N y_n\\log \\hat{y}_n$, __find the derivative__ $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$ and use it to complete the code in  `softmax_cross_entropy_backward`.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- In `softmax_cross_entropy_backward` the softmax function and the cross-entropy function are combined for the ease of differentiation.\n",
        "- When differentiating the loss function with respect to $z_j$ consider the cases $i=j$ and $i\\ne j$, where $i$ is the index of the correct class."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "qotFmjm_0rop",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax_cross_entropy_backward(z, y_true):\n",
        "    \"\"\"\n",
        "        Compute gradients of loss with respect to linear outputs z and true labels (y_true).\n",
        "        This is the gradients produced by the softmax and cross entropy functions combined.\n",
        "\n",
        "        z: np.array, shape=[m, k]\n",
        "        y_true: np.array, shape=[m, k] one-hot encoded labels\n",
        "\n",
        "        return: gradient of loss with respect to z\n",
        "    \"\"\"\n",
        "    # place your code here\n",
        "\n",
        "    y_true = y_true.argmax(axis=1)\n",
        "    m = y_true.shape[0]\n",
        "    grad = softmax(z) \n",
        "    grad[range(m), y_true] -= 1\n",
        "    grad = grad/m\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jfZh_qn7Bh_J",
        "colab_type": "code",
        "outputId": "6c5f7a2e-cc61-44c6-a67f-05b78520c07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "z_grad_toy = softmax_cross_entropy_backward(z_toy, y_true_toy)\n",
        "print(z_grad_toy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.04761581 -0.2268229   0.00491929]\n",
            " [-0.23024977  0.02364531  0.01197912]\n",
            " [ 0.0149269  -0.23551426  0.01507692]\n",
            " [ 0.01600922  0.02774803 -0.21933368]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "frfWe-_90ror",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.6\n",
        "#### (3 points)\n",
        "\n",
        "Knowing that $\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}$, find the gradient of the loss with respect to each input of the linear function: $\\mathbf{x}$, $\\mathbf{W}$ and $\\mathbf{b}$. These would be $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}$, $\\frac{\\partial \\mathcal{L}}{\\partial W}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}$. For now, we only need the gradients of $\\mathbf{W}$ and $\\mathbf{b}$ for training our linear model. Later, we will need the gradient of the linear input $\\mathbf{x}$ in order to calculate gradients for multiple layers.\n",
        "\n",
        "\n",
        "Complete the code in `linear_backward` (given that you already computed $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$). This function computes the target gradients $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}$, $\\frac{\\partial \\mathcal{L}}{\\partial W}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}$.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- `z_grad` in the function below denotes $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}$ that you computed in the previous function.\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ZlYzJU9y0ror",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_backward(x, W, z_grad):\n",
        "    \"\"\"\n",
        "        Compute gradients for loss function with respect to inputs x, W, and b given z_grad (dL/dz).\n",
        "\n",
        "        x: np.array, shape=[m, d] input\n",
        "        W: np.array, shape=[d, k] weights\n",
        "        z_grad: np.array, shape=[m, k] gradient of loss with respect to output z\n",
        "\n",
        "        return: dL/dx: np.array, shape=[m, d],\n",
        "                dL/dW: np.array, shape=[d, k],\n",
        "                dL/db: np.array, shape=[1, k]\n",
        "    \"\"\"\n",
        "     \n",
        "    dzdx = W.transpose()\n",
        "    dzdw = x\n",
        "    dldx = np.dot(z_grad,dzdx)\n",
        "    dldw = np.dot((z_grad.transpose()), dzdw).transpose()\n",
        "    ones = np.ones((1, x.shape[0])).transpose()\n",
        "    dldb = np.dot(z_grad.transpose(),ones).transpose()\n",
        "    return dldx, dldw, dldb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdPLrwoG2q7S",
        "colab_type": "code",
        "outputId": "cb062b09-210e-4cb3-8e2f-d397f29f4398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "linear_backward(x_toy, W_toy, z_grad_toy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.08002968, -0.17698025, -0.1549983 ,  0.00441141,  0.05896494],\n",
              "        [-0.04715399,  0.22620363, -0.00886908, -0.13937907, -0.1563519 ],\n",
              "        [ 0.08117812, -0.148712  , -0.16638267, -0.01911836,  0.04304641],\n",
              "        [-0.11478941, -0.08549295,  0.06489128,  0.07818044, -0.12316856]]),\n",
              " array([[-0.15164248,  0.23174295, -0.12902225],\n",
              "        [ 0.02392534,  0.17347059, -0.15937291],\n",
              "        [-0.12663404,  0.04589563, -0.04123424],\n",
              "        [ 0.03820088, -0.15513209, -0.01950563],\n",
              "        [ 0.09551822,  0.04874394, -0.0955405 ]]),\n",
              " array([[-0.15169784, -0.41094381, -0.18735835]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "TKmqg-nqkdYm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "97NQhGUq0ros",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.7\n",
        "#### (3 points)\n",
        "\n",
        "Complete the code in the `update_weights` function using the gradients obtained in the previous step. Recall that, according to a single step of the gradient descent algorithm, the parameters are updated as follows:\n",
        "\n",
        "$$ W_{ij} \\leftarrow W_{ij} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} $$\n",
        "\n",
        "$$ b_{i} \\leftarrow b_{i} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial b_{i}} $$\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- The weights initialization is provided for you.\n",
        "- Inside the `update_weights` function you can call the previously implemented functions."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "7aVGVYAm0ros",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_params(x, y_true):\n",
        "    \"\"\"Given x and y pairs from the dataset, initialize parameters for the linear network.\n",
        "    \n",
        "    x: np.array, shape=[m, d]\n",
        "    y_true: np.array, shape=[m, k]\n",
        "    \n",
        "    Return: list of np.array parameters\"\"\"\n",
        "    n_features = x.shape[1]\n",
        "    n_classes = y_true.shape[1]\n",
        "\n",
        "    # Initialize W from normal distribution and b with zeros\n",
        "    W = np.random.randn(n_features, n_classes) / n_features\n",
        "    b = np.zeros(shape = (1, n_classes))\n",
        "    return [W, b]\n",
        "\n",
        "def update_weights(params, x, y_true, alpha, lam=0.05, record_values=False):\n",
        "    \"\"\"\n",
        "        Update network weights as a step of gradient descent. This function should calculate gradients with respect\n",
        "        to the cross entropy loss, as well as the L2 weight regularization, then apply these gradients to the model\n",
        "        parameters.\n",
        "        \n",
        "        params: list of parameters to update\n",
        "        x: np.array, shape=[m, d] input\n",
        "        W: np.array, shape=[d, k] weights\n",
        "        b: np.array, shape=[1, k] biases\n",
        "        y_true: np.array, shape=[m, k] one-hot encoded labels\n",
        "        alpha: float, learning rate\n",
        "        lam: lambda regularization constant\n",
        "        record_values: used to write values\n",
        "        \n",
        "        return: computed loss for model output (cross entropy + regularization losses)\n",
        "    \"\"\"\n",
        "    W, b = params\n",
        "    # place your code here\n",
        "    z = linear(x, W, b)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy(y, y_true, epsilon=1e-9) \n",
        "    z_grad = softmax_cross_entropy_backward(z, y_true)\n",
        "    x_grad, W_grad, b_grad = linear_backward(x, W, z_grad)\n",
        "\n",
        "    # calculate regularization\n",
        "    value, gradients = regularization([W, b], lam = 0.05)\n",
        "    W_reg_grad, b_reg_grad = gradients[0], gradients[1]\n",
        "    reg_loss = value\n",
        "    W -= alpha * (W_grad + W_reg_grad)\n",
        "    b -= alpha * (b_grad + b_reg_grad)\n",
        "\n",
        "    if record_values:\n",
        "        pm.record('z.toy', z.tolist())\n",
        "        pm.record('y.toy', y.tolist())\n",
        "        pm.record('loss.toy', loss)\n",
        "\n",
        "        pm.record('z_grad.toy', z_grad.tolist())\n",
        "        pm.record('x_grad.toy', x_grad.tolist())\n",
        "        pm.record('W_grad.toy', W_grad.tolist())\n",
        "        pm.record('b_grad.toy', b_grad.tolist())\n",
        "    \n",
        "    return loss + reg_loss\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "YnxDgFam0rov",
        "colab_type": "code",
        "outputId": "4205bd74-8d9b-4832-8106-b1f5a0fdea1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "cell_type": "code",
      "source": [
        "# submit\n",
        "loss = update_weights([W_toy, b_toy], x_toy, y_true_toy, 0.1, record_values=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "z.toy": [
                [
                  0.9825232604650328,
                  0.3547453144542251,
                  -1.2697250159441542
                ],
                [
                  0.14354918552394935,
                  0.3192726650084948,
                  -0.36544807560100634
                ],
                [
                  -0.15438677952521493,
                  -0.15796366570014755,
                  -0.14212917822867432
                ],
                [
                  -0.07807649811371065,
                  0.4592305136597842,
                  0.5869746352007815
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "y.toy": [
                [
                  0.184386326102342,
                  0.0984210996987004,
                  0.0193905314726649
                ],
                [
                  0.07968322445170167,
                  0.0949910388691681,
                  0.04789742853032197
                ],
                [
                  0.05915275198821154,
                  0.05894154728019716,
                  0.05988228486498116
                ],
                [
                  0.06384341190471897,
                  0.10926109753774425,
                  0.12414925729924796
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "loss.toy": 2.4414190052297684
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "z_grad.toy": [
                [
                  0.0460965815255855,
                  -0.2253947250753249,
                  0.004847632868166225
                ],
                [
                  -0.2300791938870746,
                  0.023747759717292025,
                  0.011974357132580493
                ],
                [
                  0.014788187997052884,
                  -0.2352646131799507,
                  0.01497057121624529
                ],
                [
                  0.015960852976179742,
                  0.027315274384436063,
                  -0.218962685675188
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "x_grad.toy": [
                [
                  0.08445386374256736,
                  -0.1691599522090779,
                  -0.150956622491997,
                  -0.000035688137496191314,
                  0.057843173569095775
                ],
                [
                  -0.05056627787047191,
                  0.22417793378899997,
                  -0.011664767592075439,
                  -0.13660171350223144,
                  -0.15248681829124272
                ],
                [
                  0.08608158096157549,
                  -0.14271075138692432,
                  -0.1632097588802294,
                  -0.022630272887596493,
                  0.04358047813714634
                ],
                [
                  -0.11655500078214992,
                  -0.08870676905426171,
                  0.06303809881280614,
                  0.07715255175660357,
                  -0.12408475749101305
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "W_grad.toy": [
                [
                  -0.15008084788370968,
                  0.2301313082466375,
                  -0.12874100858536425
                ],
                [
                  0.024920828792830894,
                  0.17214744767837425,
                  -0.15906209440686406
                ],
                [
                  -0.1269416464781543,
                  0.04619913845664524,
                  -0.041141861481537555
                ],
                [
                  0.036994531718381095,
                  -0.15405779130382127,
                  -0.01951521939461035
                ],
                [
                  0.09545838114589117,
                  0.04847992850284776,
                  -0.09536893136635646
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "b_grad.toy": [
                [
                  -0.15323357138825647,
                  -0.40959630415354753,
                  -0.187170124458196
                ]
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "VxZvLU8F0row",
        "colab_type": "code",
        "outputId": "32be32e8-d362-4f3d-cda8-9b2f2bf78208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print('Loss:', loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.6202612556582965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "evtZT_M30rox",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.8\n",
        "#### (2 points)\n",
        "Recall that training of a neural network is done in iterations such that after every iteration the network produces the predictions, computes the value of the loss function based on the predictions and the ground-truth labels, calculates the loss gradients with respect to every weight, and then uses the derived gradients to update the weights.\n",
        "\n",
        "Keeping this logic in mind, complete the code in the `train` function\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- Append the mean loss of each epoch to a list to be able to plot the learning curve after the training finishes.\n",
        "- The current implementation takes the `init_func` and `update_func` as the arguments, which are going to be different for a single-layer network and a multi-layer network. This allows us to unify the training loop for both architectures."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ORT0oOrX0rox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(init_func, update_func, x, y_true, batch_size=50, num_epoch=1, learning_rate=1e-3, lam=0.001, plot_loss=True,plot_per_epoch=True):\n",
        "    \"\"\"\n",
        "        Train a neural network.\n",
        "        \n",
        "        init_func: function mapping (x, y_true) --> params\n",
        "        update_func: function mapping (list params, np.array x, np.array y_true, float alpha, float lam) --> float loss, while updating parameters\n",
        "        x_train: np.array, shape=[m, d]\n",
        "        y_train: np.array, shape=[m, k] one-hot encoding of labels\n",
        "        batch_size: int, number of examples in a batch\n",
        "        num_epoch: int, number of training iterations\n",
        "        learning_rate: float, learning rate\n",
        "        lam: float, regularization parameter\n",
        "        plot_loss: bool, optionally plot the learning curve\n",
        "        \n",
        "        return: list of losses per batch over all epochs, learned list of parameters\n",
        "    \"\"\"\n",
        "\n",
        "    losses = []\n",
        "    batch_losses = []\n",
        "    \n",
        "    n_batches = x.shape[0] // batch_size\n",
        "    params = init_func(x, y_true)\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        for iter_num, (x_batch, y_batch) in enumerate(zip(np.array_split(x, n_batches), np.array_split(y_true, n_batches))):\n",
        "            # place your code here\n",
        "            loss = update_weights(params, x_batch, y_batch, learning_rate, lam=0.05, record_values=False)\n",
        "            batch_losses.append(loss)\n",
        "            \n",
        "        epoch_loss = np.mean(batch_losses)\n",
        "        losses.append(epoch_loss)\n",
        "        print(batch_losses)\n",
        "        batch_losses = []\n",
        "        print(f'Epoch: {epoch:<5} Loss: {epoch_loss:.3f}')\n",
        "        \n",
        "            \n",
        "    # draw learning curve \n",
        "    if plot_loss:\n",
        "        plt.plot(losses)\n",
        "        plt.title(\"Loss\")\n",
        "        \n",
        "        if plot_per_epoch:\n",
        "            plt.xlabel(\"epochs\")\n",
        "        else:\n",
        "            plt.xlabel(\"batches\")\n",
        "           \n",
        "        plt.show()\n",
        "        \n",
        "    return losses, params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "BGgjFxQ40roy",
        "colab_type": "code",
        "outputId": "a5352c22-46a1-4195-da79-93100b18b14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# DO NOT MODIFY THIS CELL\n",
        "losses_toy, params = train(init_params, update_weights, x_toy, y_true_toy, batch_size=1, num_epoch=10, learning_rate=1)\n",
        "W_learned, b_learned = params\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.251293327396639, 1.441371588500629, 1.2599384477869355, 1.9843157066230708]\n",
            "Epoch: 0     Loss: 1.484\n",
            "[0.3339263543218267, 1.3242395298126357, 1.108011446476738, 1.0865774918883502]\n",
            "Epoch: 1     Loss: 0.963\n",
            "[0.42730775725689085, 1.1352806598659047, 1.0568766150298932, 0.9268602556114954]\n",
            "Epoch: 2     Loss: 0.887\n",
            "[0.46737547911155874, 1.0576292830051626, 1.020750840617913, 0.8934174274613688]\n",
            "Epoch: 3     Loss: 0.860\n",
            "[0.48478991360428747, 1.0325024874522009, 1.0029641189369585, 0.8830887564511067]\n",
            "Epoch: 4     Loss: 0.851\n",
            "[0.49271380447974694, 1.0235236750365986, 0.9945399149396204, 0.8792189561606567]\n",
            "Epoch: 5     Loss: 0.847\n",
            "[0.49638659011227304, 1.0200833338373834, 0.9905109759441277, 0.8776005412138741]\n",
            "Epoch: 6     Loss: 0.846\n",
            "[0.498107669678055, 1.018714575176216, 0.9885578072957641, 0.8768645714415402]\n",
            "Epoch: 7     Loss: 0.846\n",
            "[0.49892401953663446, 1.0181585114322949, 0.9875994365822421, 0.8765054928623275]\n",
            "Epoch: 8     Loss: 0.845\n",
            "[0.4993164802961681, 1.0179302023411279, 0.9871241619118492, 0.8763201470591058]\n",
            "Epoch: 9     Loss: 0.845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEVCAYAAAD91W7rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "GteThzOH0ro0",
        "colab_type": "code",
        "outputId": "84721891-75ea-485a-caea-3af1dc571dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# submit\n",
        "pm.record('losses.toy', losses_toy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.0). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "losses.toy": [
                1.4842297675768186,
                0.9631887056248877,
                0.886581321941046,
                0.8597932575490008,
                0.8508363191111383,
                0.8474990876541556,
                0.8461453602769144,
                0.8455611558978938,
                0.8452968651033748,
                0.8451727479020628
              ]
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "0xqIr6ot0ro1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have the `train` function, we can train our network on the 20 Newsgroups dataset."
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "Z9KlVepn0ro2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.9\n",
        "#### (1 point)\n",
        "\n",
        "Now we can test the implemented functions on our real dataset.\n",
        "\n",
        "In the cell below, create the `X` and `y_true` matrices. During construction of the vocabulary, discard all tokens that appear less than 10 times (set the `min_count` argument to `10`)\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- You can use the functions `tokenize_doc`, `build_vocab`, `doc_to_multihot`, and `labels_to_onehot`\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "AxcODxX90ro2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create X and y_true here\n",
        "docs_tokenized = [tokenize_doc(d) for d in dataset.data]\n",
        "docs_vocab = build_vocab(docs_tokenized, min_count = 10)\n",
        "docs_multihot = [doc_to_multihot(doc, docs_vocab) for doc in docs_tokenized]\n",
        "docs_labels = labels_to_onehot(dataset.target_names)\n",
        "x = docs_multihot\n",
        "y_true = docs_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "IMy1AClU0ro5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('X:', x.shape)\n",
        "print('y_true:', y_true.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "2K6s5sFZ0ro6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('x.shape', x.shape)\n",
        "pm.record('x.max', x.sum(axis=1).max())\n",
        "pm.record('x.min', x.sum(axis=1).min())\n",
        "pm.record('x.mean', x.sum(axis=1).mean())\n",
        "pm.record('y_true.shape', y_true.shape)\n",
        "pm.record('y_true.max', y_true.max())\n",
        "pm.record('y_true.min', y_true.min())\n",
        "pm.record('y_true.mean', y_true.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "yFwAzZI20ro8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.10 \n",
        "#### (2 points)\n",
        "\n",
        "Using the implemented `train` function, now train a neural network on the Newsgroups dataset. Once the network is trained, complete the code in the `make_prediction` function that makes predictions for given inputs.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- Recall that $\\hat{y}_{pred} = \\underset{j \\in 1..k}{\\text{argmax}}\\left[\\text{softmax}\\left(\\mathbf{z}\\right)\\right]$\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "1rHh1mxT0ro8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "losses, params = train(init_params, update_weights, x, y_true, batch_size=128, num_epoch=20, learning_rate=0.1, plot_loss=True)\n",
        "W_learned, b_learned = params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "c75dAvRj0ro9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('losses', losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "9MLOXHz40ro-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_prediction(x, W, b):\n",
        "    \"\"\"\n",
        "        Make predictions for the given inputs x.\n",
        "\n",
        "        x: np.array [N, d], inputs\n",
        "        W: np.array, shape=[d, k], weights\n",
        "        b: np.array, shape=[1, k], biases\n",
        "\n",
        "        return: np.array of labels with shape [N]\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    z = linear(x, W, b)\n",
        "    pred = np.argmax(softmax(z), axis = 0) \n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "XcfOtHkj0ro_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = make_prediction(x, W_learned, b_learned)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "SEI7rnwV0rpA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('y_pred.shape', y_pred.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "SFJtlUUY0rpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(dataset.target, y_pred)\n",
        "print('Accuracy:', accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "dMz070e-0rpC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('accuracy', accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "BqiYJy4x0rpD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 Let's add more layers!"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "dmdbqxZQ0rpE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Similarly to the single-layer neural network, the two-layer neural network makes the following transformations:\n",
        "$$ \\mathbf{z}^{(1)} = W^{(1)}\\mathbf{x} + \\mathbf{b^{(1)}} $$\n",
        "$$ \\mathbf{a}^{(1)} = f\\left(\\mathbf{z}^{(1)}\\right)$$\n",
        "\n",
        "$$ \\mathbf{z}^{(2)} = W^{(2)}\\mathbf{a^{(1)}} + \\mathbf{b^{(2)}} $$\n",
        "$$ \\mathbf{\\hat{y}} = \\text{softmax}\\left(\\mathbf{z}^{(2)}\\right)$$\n",
        "where $f$ is the activation of the first linear layer. In our network we will use the Rectified Linear Unit (ReLU) activation function defined as:\n",
        "\n",
        "$$ f(x)=\\text{ReLU(x)} = \\max(0,x) $$\n",
        "\n",
        "The corresponding formulas for computing the loss gradients are then as follows:\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial  \\mathbf{z}^{(2)}}{\\partial W^{(2)}}$$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{b}^{(2)}}$$\n",
        "\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial \\mathbf{z}^{(1)}} \\cdot \\frac{\\partial\\mathbf{z}^{(1)}}{\\partial W^{(1)}}  $$\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial \\mathbf{z}^{(1)}} \\cdot \\frac{\\partial\\mathbf{z}^{(1)}}{\\partial \\mathbf{b}^{(1)}}  $$\n",
        "\n",
        "In these notations $W^{(1)} \\in \\mathbb{R}^{m\\times d_2}$, $W^{(2)} \\in \\mathbb{R}^{d_2\\times k}$,  $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{d_2}$ and $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{k}$.\n"
      ]
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "Ew2tLM0Z0rpE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1 \n",
        "#### (2 points)\n",
        "\n",
        "Simlarly to sections 3.2 and 3.6, perform the forward and backward propagation pass through the ReLU function (using its' definition).\n",
        "\n",
        "---\n",
        "\n",
        "__Notes__:\n",
        "- In `relu_backward` the variable `a_grad` stands for  $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}} \\cdot \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a^{(1)}}$\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "_ooWYN5K0rpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    \"\"\"\n",
        "        Compute relu activation as max(z, 0)\n",
        "        \n",
        "        z: np.array, shape=[m, d]\n",
        "\n",
        "        Return: np.array, shape=[m, d]\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    fun = np.maximum(0, z)\n",
        "    return fun"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "0_m8hrF10rpF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def relu_backward(z, a_grad):\n",
        "    \"\"\"\n",
        "        Return gradient of loss with respect to input z given the output gradient.\n",
        "\n",
        "        z: np.array, shape=[m, d] input to relu, not the output\n",
        "        \n",
        "        return: gradient dL/dz np.array, shape=[m, d] //dL/dz1\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    a = relu(z)\n",
        "    a[a>0]=1\n",
        "    dLdz = np.dot(a_grad,a)\n",
        "    return dLdz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "OsKrtJHg0rpG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2 \n",
        "#### (1 point)\n",
        "\n",
        "Perform the forward propagation of the two-layer neural network by completing the code in `neural_network`"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "-PVMsDlS0rpH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neural_network(x, W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "        Use layers created above to define two-layer neural network with relu activation.\n",
        "\n",
        "        x: np.array, shape=[m, n_features_tst]\n",
        "        W1: np.array, shape=[m, d2] first layer weights\n",
        "        W2: np.array, shape=[d2, k] second layer weights\n",
        "        b1: np.array, shape=[1, d2] first layer bias\n",
        "        b2: np.array, shape=[1, k] second layer bias\n",
        "\n",
        "        return: tuple, output of the linear layer, output of the relu, output of the final linear layer\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    z1 = linear(x, W1, b1)\n",
        "    z2 = relu(z)\n",
        "    z3 = linear(z2, W2, b2)\n",
        "    return z1, z2, z3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "OLZ353CP0rpI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.3\n",
        "#### (3 points)\n",
        "\n",
        "Compute the code in `neural_network_backward` that computes gradients of the loss function with respect to every weight.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- You should call the already implemented backward functions in the cell below."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "XPJNN6Fh0rpI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neural_network_backward(x, W1, b1, W2, b2, z1, a1, z2, z2_grad):\n",
        "    \"\"\"\n",
        "        Compute gradients of loss with respect to inputs x, W1, b1, W2, and b2.\n",
        "\n",
        "        x: np.array, shape=[m, d] input\n",
        "        W1: np.array, shape=[m, d2] first layer weights\n",
        "        b1: np.array, shape=[1, d2] first layer bias\n",
        "        W2: np.array, shape=[d2, k] second layer weights\n",
        "        b2: np.array, shape=[1, k] second layer bias\n",
        "\n",
        "        z1, z2, z3: np.array activations of linear, relu, and linear layers\n",
        "        z3_grad: np.array, shape=[m, k] gradient of loss with respect to network output dL/dz3\n",
        "\n",
        "        return: tuple: gradients dL/dW1, dL/db1, dL,dW2, dL/db2\n",
        "    \"\"\"\n",
        "    a1_grad, W2_grad, b2_grad = None, None, None\n",
        "    z1_grad = None\n",
        "    x_grad, W1_grad, b1_grad = None, None, None\n",
        "\n",
        "    return W1_grad, b1_grad, W2_grad, b2_grad\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "LczU9eGk0rpK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.4\n",
        "#### (3 points)\n",
        "\n",
        "Complete the code and `update_neural_network_weights`.\n",
        "\n",
        "---\n",
        "__Notes__:\n",
        "- The weights initialization is provided for you.\n"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "XrEl4X9Z0rpL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_neural_network_params(x, y_true):\n",
        "    \"\"\"\n",
        "        Initialize weights and biases of two layer neural network, given input data.\n",
        "        \n",
        "        x: np.array, shape=[m, d] input\n",
        "        y_true: np.array, shape=[m, k] true label one-hot\n",
        "        \n",
        "        return: list of np.array parameters\n",
        "    \"\"\"\n",
        "    n_features = x.shape[1]\n",
        "    n_classes = y_true.shape[1]\n",
        "    n_middle = n_classes * 4  # we arbitrarily pick the middle layer size to be 4 times bigger than the output\n",
        "\n",
        "    # Initialize W from normal distribution and b with zeros\n",
        "    W1 = np.random.randn(n_features, n_middle) / n_features\n",
        "    W2 = np.random.randn(n_middle, n_classes) / n_middle\n",
        "    b1 = np.zeros(n_middle)\n",
        "    b2 = np.zeros(n_classes)\n",
        "    \n",
        "    return [W1, W2, b1, b2]\n",
        "\n",
        "def update_neural_network_weights(params, x, y_true, alpha, lam=0.001):\n",
        "    \"\"\"\n",
        "        Given neural network parameters, update parameters and return loss.\n",
        "        \n",
        "        params: list of np.array parameters\n",
        "        x: np.array, shape=[m, d] input\n",
        "        y_true: np.array, shape=[m, k] output labels one-hot\n",
        "        alpha: learning rate (float)\n",
        "        lam: lambda regularization constant (float)\n",
        "        \n",
        "        return: float sum of cross entropy loss and L2 regularization losses\n",
        "    \"\"\"\n",
        "    # place your code below\n",
        "    W1, W2, b1, b2 = params\n",
        "    z1, a1, z2 = None, None, None\n",
        "    y = None\n",
        "    loss = None\n",
        "    z2_grad = None\n",
        "    W1_grad, b1_grad, W2_grad, b2_grad = None, None, None, None\n",
        "    reg_loss, [W1_reg_grad, W2_reg_grad, b1_reg_grad, b2_reg_grad] = None, [None, None, None, None]\n",
        "    # place your code above\n",
        "    W1 -= alpha * (W1_grad + W1_reg_grad)\n",
        "    W2 -= alpha * (W2_grad + W2_reg_grad)\n",
        "    b1 -= alpha * (b1_grad + b1_reg_grad)\n",
        "    b2 -= alpha * (b2_grad + b2_reg_grad)\n",
        "    return loss + reg_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "L1OIHVhs0rpM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.5\n",
        "#### (1 point)\n",
        "Complete the code in `make_prediction_network` function similarly to the section 3.10."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "gTad1MXa0rpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_prediction_network(x, W1, b1, W2, b2):\n",
        "    \"\"\"\n",
        "        Make predictions for the given inputs x.\n",
        "\n",
        "        x: np.array, shape=[m, d] input\n",
        "        W1: np.array, shape=[m, d2] first layer weights\n",
        "        b1: np.array, shape=[1, d2] first layer bias\n",
        "        W2: np.array, shape=[d2, k] second layer weights\n",
        "        b2: np.array, shape=[1, k] second layer bias\n",
        "\n",
        "        return: np.array of labels with shape [N]\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "K7t3JbOf0rpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test the neural network on the toy dataset\n",
        "losses_nn, params_nn = train(init_neural_network_params, update_neural_network_weights, x_toy, y_true_toy, batch_size=1, num_epoch=1000, learning_rate=0.1, plot_loss=True, lam=0.001, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "scrolled": true,
        "id": "QOoYrsKn0rpP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.6 Let's train!"
      ]
    },
    {
      "metadata": {
        "id": "tTphoP5H0rpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now, we train on the real dataset!\n",
        "losses_nn, params_nn = train(init_neural_network_params, update_neural_network_weights, x, y_true, batch_size=128, num_epoch=20, learning_rate=0.1, plot_loss=True, lam=0.0, plot_per_epoch=False)\n",
        "W1_learned_nn, W2_learned_nn, b1_learned_nn, b2_learned_nn = params_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "fF4ff3TL0rpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('losses_nn', losses_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "UHCyRCjn0rpT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred_nn = make_prediction_network(x, W1_learned_nn, b1_learned_nn, W2_learned_nn, b2_learned_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nEB-ZAp50rpU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('y_pred_nn.shape', y_pred_nn.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "pv2Sc2Vq0rpV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "accuracy_nn = accuracy_score(dataset.target, y_pred_nn)\n",
        "print('Accuracy NN:', accuracy_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "H0cWIudX0rpX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "pm.record('accuracy.nn', accuracy_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "mcuE6KfY0rpY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}